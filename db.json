{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"2c09f81a01ff83d19209474564650d16f298f40f","modified":1521805659509},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1520598006265},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1520598006265},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1520598006267},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1520598006265},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1520598006267},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1520598006267},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1520598006267},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1520598006267},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1520598006267},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1520598006268},{"_id":"themes/next/README.cn.md","hash":"5d8af3d8de8d3926126a738519e97c8442b0effe","modified":1520598006268},{"_id":"themes/next/README.md","hash":"44b28d995681a7c48bfe3d0577d6203812d07e59","modified":1520598006268},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1520598006269},{"_id":"themes/next/_config.yml","hash":"c08eaf281eb4dcd52d79806318a906df89d56945","modified":1520665264309},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1520598006269},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1520598006288},{"_id":"source/_posts/.DS_Store","hash":"b98f7652928712ae45e4840fa08f45df7fe3f020","modified":1521805659508},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks.md","hash":"54dadce931065b5513668b2ff4691afa3074fba9","modified":1520688728864},{"_id":"source/_posts/Social-LSTM.md","hash":"3d4b1dd694288bc5dbae145243b73af5b5704702","modified":1520690545000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015.md","hash":"c666ac84785e4718c8941f0a046c2f3ab8936818","modified":1520688516426},{"_id":"source/_posts/You-ll-never-walk-alone.md","hash":"1228552aec9145f12c3532b776c0f0cb8cece265","modified":1521805838252},{"_id":"source/tags/index.md","hash":"678d5078ece8b4de74ed312f6ae0b4100e2f3ae2","modified":1520686358566},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1520598006250},{"_id":"themes/next/.git/config","hash":"bf7d1df65cf34d0f25a7184a58c37a09f72e4be7","modified":1520598006252},{"_id":"themes/next/.git/index","hash":"d112c2d7e092d7b55d2a79d8da1b24ac02a3930c","modified":1520690092063},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1520597616008},{"_id":"themes/next/.git/packed-refs","hash":"339779e225d913a344c5e6210617badd049c4434","modified":1520598006248},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1520598006266},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1520598006265},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1520598006266},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1520598006266},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1520598006270},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1520598006269},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1520598006269},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1520598006270},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1520598006270},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1520598006270},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1520598006270},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1520598006271},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1520598006271},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1520598006271},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1520598006271},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1520598006272},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1520598006272},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1520598006272},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1520598006272},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1520598006272},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1520598006273},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1520598006287},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1520598006287},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1520598006287},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1520598006288},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1520598006288},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1520598006288},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1520598006289},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1520598006288},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1520598006289},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1520598006381},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1520598006381},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1520598006381},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520598006315},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198049016180.jpg","hash":"29f3cc217e97cf61cfe249bf122ee918ba51c228","modified":1519804901000},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198064201861.jpg","hash":"bbfe2ad293edc79d2676d8f9d0f04a40d52d699f","modified":1519806420000},{"_id":"source/_posts/Social-LSTM/15197392983551.jpg","hash":"6a1d7ddd25100707799d413a2f91914ec0b51a66","modified":1519739298000},{"_id":"source/_posts/Social-LSTM/15197416749439.jpg","hash":"7c4181fe6ef192556a72f9d52159657a21e74492","modified":1519741674000},{"_id":"source/_posts/Social-LSTM/15197428867541.jpg","hash":"920fd6bad8a7bec8de320a1d748f82045a71e88b","modified":1519742886000},{"_id":"source/_posts/Social-LSTM/15197479345650.jpg","hash":"e9fd36cba422dcc5a17d08da835414fde1748b49","modified":1519747934000},{"_id":"source/_posts/Social-LSTM/15197501486751.jpg","hash":"dcdecae7d2d493b8b017db821b796f57e61e04d4","modified":1519750148000},{"_id":"source/_posts/Social-LSTM/15197514964871.jpg","hash":"940868db14ab1edeb1240dc6e98232d174d66086","modified":1519751496000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198897069641.jpg","hash":"bb58eaacbe7c9daf3fec3587e5f8e5d04dd2363d","modified":1519889706000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198918330798.jpg","hash":"1d12fe3157229613316aec961907f6fc04f13e92","modified":1519891833000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198919545235.jpg","hash":"1cd742da2e5170441d69fcb971d160f583d7248e","modified":1519891954000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198927017157.jpg","hash":"f5ee1d6129b6798ab19a1a6e6b7e2ed5e8eebb9a","modified":1519892701000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15199214092427.jpg","hash":"1fc30417a9dedd5a0e195f0dcdfef784e1e2f194","modified":1519921409000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15199222476098.jpg","hash":"f4f776178e026b7541755b129089bfdbda690aff","modified":1519922247000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15199222658062.jpg","hash":"869038b47d6f6e909e8054adaaa1c76e4ccf933a","modified":1519922265000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201623680549.jpg","hash":"2607ac92ce0987869ed541345aa22690952e58cb","modified":1520162368000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201632366350.jpg","hash":"cbaef412e6f4a0cd8d2a12b775270b9a20b1f6e8","modified":1520163236000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201639578418.jpg","hash":"e806f0b0fb6aaeee5bbedbdae0b0defecff20bb6","modified":1520163957000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201640415371.jpg","hash":"4ff3efa8c9b4daa4679d66628a8583a027d7fe3e","modified":1520164041000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201640785872.jpg","hash":"8ef5f07b468bbf5e1bd4b8a8e8f10189ee9667e0","modified":1520164078000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201654272269.jpg","hash":"529ca0b47fc8f8226e41899b3251709aeedec075","modified":1520165427000},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201663728707.jpg","hash":"418aaf7e3e5d7d394ce5f3f814b876e6b63463e0","modified":1520166372000},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1520597616009},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1520597616008},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1520597616009},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1520597616010},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1520597616010},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1520597616009},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1520597616009},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"18be3eb275c1decd3614e139f5a311b75f1b0ab8","modified":1520597616008},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1520597616009},{"_id":"themes/next/.git/logs/HEAD","hash":"a971a594a18f1d5cccaece77cd8dcce7e7d2c7ec","modified":1520598006251},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1520597616008},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1520597616010},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1520598006273},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1520598006273},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1520598006274},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1520598006274},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1520598006274},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1520598006274},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1520598006275},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1520598006275},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1520598006275},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1520690087081},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1520598006275},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1520598006276},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1520598006276},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1520598006276},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1520598006277},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1520598006278},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1520598006279},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1520598006280},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1520598006284},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1520598006285},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1520598006285},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1520598006285},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1520598006285},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1520598006285},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1520598006285},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1520598006290},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1520598006290},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1520598006290},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1520598006290},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1520598006290},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1520598006291},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1520598006291},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1520598006291},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1520598006291},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1520598006315},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1520598006316},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1520598006316},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1520598006316},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1520598006316},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1520598006317},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1520598006317},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1520598006318},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1520598006317},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1520598006318},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1520598006317},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1520598006319},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1520598006318},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1520598006318},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1520598006319},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1520598006319},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1520598006319},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1520598006320},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1520598006320},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198287607930.jpg","hash":"e8f254d395863ebcd8ba5c7b2c2a9e9a1b6d44b0","modified":1519828760000},{"_id":"source/_posts/Social-LSTM/15197406060633.jpg","hash":"2ee350a2d6927e403ff03ea6cb4aec93b95ced7c","modified":1519740606000},{"_id":"source/_posts/Social-LSTM/15197501740806.jpg","hash":"8e3470863a2018f650d9f74459124869587c5454","modified":1519750174000},{"_id":"source/_posts/Social-LSTM/15203542098731.jpg","hash":"b2f99157130e9689389cd63d5d85de2ce8b0cd15","modified":1520354209000},{"_id":"source/_posts/Social-LSTM/Snipaste_2018-02-28_00-50-47.png","hash":"fa52d6aa9148f6237a49ccb00d75fbea643c54ea","modified":1519751112000},{"_id":"source/_posts/You-ll-never-walk-alone/15214261731616.jpg","hash":"7650bffd18558b1a5d8bf3845ff40105920d15cc","modified":1521426173170},{"_id":"source/_posts/You-ll-never-walk-alone/15214282365054.jpg","hash":"889924d50b801aedbd1796c5228a38e64ee621fa","modified":1521428236520},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201667242091.jpg","hash":"1e1e1f88460dca1a0b3082f628ed3b84ed67fe17","modified":1520166724000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520598006279},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520598006279},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520598006309},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520598006309},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520598006309},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520598006314},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1520675353798},{"_id":"source/_posts/Social-LSTM/15197514383605.jpg","hash":"70a41b44e1b043de5e9f2235e329ec835dc1dced","modified":1519751438000},{"_id":"themes/next/.git/refs/heads/master","hash":"f4d9f6f8bc79e9bc071cf29324a74a1d78158ab9","modified":1520598006251},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1520598006276},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1520598006276},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1520598006277},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1520598006277},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1520598006277},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1520598006277},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1520598006278},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1520598006278},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1520598006278},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1520598006279},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1520598006279},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1520598006280},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1520598006280},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1520598006280},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1520598006281},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1520598006281},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1520598006281},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1520598006281},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1520598006281},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1520598006282},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1520598006282},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1520598006282},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1520598006282},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1520598006282},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1520598006283},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1520598006283},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1520598006283},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1520598006283},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1520598006283},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1520598006284},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1520598006284},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1520598006284},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1520598006284},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1520598006284},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1520598006287},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1520598006286},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1520598006286},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1520598006286},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"c4ece3d8ab251a22dbe0cf68939c58405beec07a","modified":1520676536000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1520598006309},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1520598006309},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1520598006314},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1520598006309},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1520598006314},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1520598006315},{"_id":"themes/next/source/css/_variables/base.styl","hash":"9c57e5153abb991657bc438b4a16933d799075db","modified":1520675494576},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1520598006321},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1520598006320},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1520598006320},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1520598006321},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1520598006321},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1520598006321},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1520598006322},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1520598006322},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1520598006322},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1520598006323},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1520598006323},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1520598006327},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1520598006331},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1520598006331},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1520598006332},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1520598006336},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1520598006331},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1520598006336},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1520598006336},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1520598006338},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1520598006338},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1520598006337},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1520598006338},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1520598006339},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1520598006339},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1520598006354},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1520598006356},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1520598006356},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1520598006363},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1520598006357},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1520598006363},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1520598006362},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1520598006364},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1520598006366},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1520598006365},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1520598006366},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1520598006366},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1520598006367},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1520598006367},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1520598006368},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1520598006367},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1520598006368},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1520598006368},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1520598006369},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1520598006369},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1520598006369},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1520598006369},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1520598006369},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1520598006370},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1520598006370},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1520598006371},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1520598006371},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1520598006376},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1520598006376},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1520598006380},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197998463524.jpg","hash":"8c50ef0e2c915613a1bdab603240d7f2985eaf0c","modified":1519799846000},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197889166956.jpg","hash":"5a579266da05ec26e5ae831a3ff0d56060591e4d","modified":1519788916000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1520598006379},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201660409081.jpg","hash":"7477d52ee80a1799d6ea3ee769976c435ed959ba","modified":1520166040000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1520598006380},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1520598006355},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197894945788.jpg","hash":"4218b72ade6462cc7f5048043cd5780d201cd2ef","modified":1519789494000},{"_id":"source/_posts/Social-LSTM/15197217698948.jpg","hash":"2eeeee873e858b1f73547bb55fa667946f05ada3","modified":1519721769000},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"a971a594a18f1d5cccaece77cd8dcce7e7d2c7ec","modified":1520598006251},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1520598006250},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1520598006286},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1520598006286},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1520598006292},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1520598006292},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1520598006292},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1520598006292},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1520598006293},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1520598006296},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1520598006301},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1520598006306},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1520598006307},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1520598006307},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1520598006307},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1520598006307},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1520598006308},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1520598006308},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1520598006310},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1520598006310},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1520598006310},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"91bd683c1fab6191460ea932ed02a6d456170272","modified":1520677970000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"3f50cd3c3a264661e3b72350b5159d89f86af496","modified":1520667386649},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1520598006311},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1520598006311},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a347efe88611fa40cd54875a46e55ffe0c97a8b8","modified":1520667314297},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1520598006312},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1520598006312},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4aac01962520d60b03b23022ab601ad4bd19c08c","modified":1520598006312},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1520598006312},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1520598006312},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1520598006313},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1520598006313},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1520598006313},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1520598006313},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1520598006314},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1520598006314},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1520598006322},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1520598006325},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1520598006332},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1520598006332},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1520598006327},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1520598006327},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1520598006332},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1520598006333},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1520598006333},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1520598006335},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1520598006333},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1520598006335},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1520598006335},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1520598006337},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1520598006338},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1520598006340},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1520598006340},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1520598006341},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1520598006375},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1520598006376},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198088088086.jpg","hash":"0d5a895ff6ca5892171a7c225e32837264cdc9a9","modified":1519808808000},{"_id":"source/_posts/You-ll-never-walk-alone/15214628479966.jpg","hash":"9bda3365d8702f401c5347848c8daabd02ddde7b","modified":1521462848030},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201658922276.jpg","hash":"19d4868f603f16d85f30142abf8ce69c8e1de333","modified":1520165892000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1520598006326},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1520598006354},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1520598006354},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1520598006378},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"a971a594a18f1d5cccaece77cd8dcce7e7d2c7ec","modified":1520598006250},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1520598006293},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1520675328125},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1520598006293},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1520598006294},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1520598006294},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1520598006294},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1520598006294},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1520598006293},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1520598006295},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1520598006295},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1520598006295},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1520598006295},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1520598006295},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1520598006296},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1520598006296},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1520598006296},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1520598006296},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1520598006297},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1520598006297},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1520598006297},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1520598006297},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1520598006297},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1520598006298},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1520598006298},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1520598006298},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1520598006298},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1520598006299},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1520598006298},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1520598006299},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1520598006299},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1520598006299},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1520598006299},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1520598006300},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1520598006300},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1520598006300},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1520598006300},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1520598006300},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1520598006301},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1520598006301},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1520598006301},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1520598006301},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1520598006302},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1520598006302},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1520598006302},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1520598006302},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1520598006302},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1520598006303},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1520598006303},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1520598006303},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1520598006304},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1520598006303},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1520598006304},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1520598006304},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1520598006304},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1520598006304},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1520598006305},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1520598006304},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1520598006305},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1520598006311},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1520666241241},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1520598006313},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1520598006324},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1520598006324},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1520598006325},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1520598006325},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1520598006325},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1520598006333},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1520598006334},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1520598006334},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1520598006334},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1520598006334},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1520598006334},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1520598006342},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1520598006344},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1520598006353},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198094043936.jpg","hash":"2a99794fe2831659bdc8360e3a5aac485228b4e5","modified":1519809404000},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198112216643.jpg","hash":"4e9ed3f7f16847d7af3a48c21e473bc1e620e5b7","modified":1519811221000},{"_id":"themes/next/.git/objects/pack/pack-44982fc264fff35d139337d95663cb2f8d418931.idx","hash":"167d2c701cb534d16c3c5ed866f0992307f73230","modified":1520598006227},{"_id":"source/_posts/Social-LSTM/15197511439317.png","hash":"8d9527ab3fcabae86acb02326c4983887d8e53bb","modified":1519751143000},{"_id":"source/_posts/Social-LSTM/15197511728931.png","hash":"8d9527ab3fcabae86acb02326c4983887d8e53bb","modified":1519751172000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1520598006330},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1520598006375},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1520598006349},{"_id":"themes/next/.git/objects/pack/pack-44982fc264fff35d139337d95663cb2f8d418931.pack","hash":"447b185d273f66aadc1e231dd8700f76f086454a","modified":1520598006220}],"Category":[],"Data":[],"Page":[{"title":"标签","date":"2018-03-10T07:06:23.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2018-03-10 15:06:23\ntype: \"tags\"\n---\n","updated":"2018-03-10T12:52:38.566Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjf3w0q0q000bvldxs2fdvlzl","content":"\n","site":{"data":{}},"excerpt":"","more":"\n"}],"Post":[{"title":"Pedestrian Behavior Understanding and Prediction with Deep Neural Networks","mathjax":true,"date":"2018-03-10T13:02:22.000Z","_content":"# [Pedestrian Behavior Understanding and Prediction with Deep Neural Networks](https://link.springer.com/chapter/10.1007/978-3-319-46448-0_16)\n1. 论文发表在ECCV2016，使用深层神经网络理解并预测行人运动行为。(一作：Shuai Yi)\n\n2. 论文的核心思想：通过一种编码方案(encoding scheme)，使得行人的运动轨迹适合作为CNN的输入与输出。\n\n3. 论文的贡献：\n\n    > 1. 用深层卷积网络(deep-CNN)建模了行人长期运动行为(Long-term pedestrian behaviors)，并对得到的CNN的结构进行了分析。\n\n    > 2. 提出了一个行人运动行为的编码方案(pedestrian behavior encoding scheme), 将行人的运动路径编码为“稀疏位移量”(sparse displacement volumes)。后者可以直接作为深层网络的输入和输出。\n \n4. 论文的方法：\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197889166956.jpg)\n上图所示为总体的框架: \n\n    >系统的输入为前几帧行人运动轨迹(图a 中的带颜色曲线)；然后将它们编码 为位移量(displacement volume)，如图 b 所示；将编码之后的量输入到Behavior CNN 中，网络的输出也为displacement volume，然后通过解码操作得到未来的预测估计。\n\n* 运动轨迹编码：\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197894945788.jpg)\n如上图所示为编码过程。\n\n    > 令 $p_1,p_2,...,p_N$代表场景中的$N$个行人, $t_1,t_2,...,t_m$代表$m$ 个时刻，其中$t_m$代表当前时刻。在时刻$t_m$，行人 $p_i(i\\in[1,N])$ 的归一化空间位置记为$l_i^m=[x^m_i/X, y^m_i/Y]$。其中，$x_i^m\\in[1,X]$, $y_i^m\\in[1,Y]$ 为空间坐标，$[X,Y]$ 为输入帧的空间大小(spatial size)。所有的位置都是基于网格的。\n  \n    > 使用一个 $2M$ 维的位移向量$d_i=[l_i^M-l^1_i ,l^M_i-   l_i^2,...,l_i^M-l_i^{M-1},l_i^M-l_i^M]$ 表示之前$M$帧，行人 $p_i$ 的轨迹信息。$d_i \\in (-1,1)$.\n    \n    > 基于$d_i$构建一个三维的位移量tensor $D\\in R^{X\\times Y\\times 2M}$。对于每一个行人 $p_i$ ，若其当前所在时刻的位置$(x_i^M, y_i^M)$，则令 $D(x_i^M,y_i^M,:) = d_i$。为了区分含有静止行人和没有行人的位置(格子)，对所有含有行人的位置的 $d_i+\\textbf{1}^T$。即 $D(x_i^M, y_i^M,:)=d_i+ \\textbf{1}^T$。此时，含有人的格子，其$2M$个向量元素取值范围 $\\in(0,2)$, 静止行人为 1 ，$D$ 中的其余部分均设置为 0。\n    \n* CNN 模型\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197998463524.jpg)\n\n    >1. CNN 的输入为$D\\in R^{X\\times Y \\times 2M}$，输出为 $D^* \\in R^{X\\times Y \\times 2M^*}$.\n\n    >2. 经过前三个卷积层操作之后的输出大小为 $X\\times Y \\times 64$。(为了保证feather map 的大小不变，中间有zero pad 操作)\n\n    >3. 经过max-pool 操作之后(stride 为2)，输出大小为$X/2 \\times Y/2 \\times 64$。\n\n    >4. 设置一个可学习的位置偏差图(location bias map)，其大小为$X/2 \\times Y/2$。然后将这个位置偏差图 channel-wisely (总共有64个channel)加到 pool 之后得到的feather map 上。这样，每一个位置都有一个独立的偏差值(bias)，这个值在相同位置的所有通道上共享。 这个 location bias map 表示了环境信息。\n\n    >5. 之后是三个卷基层，再接着是一个逆卷积层，得到$D^*$.\n\n* 损失函数和训练：\n损失函数计算预测得到的位移量和实际的位移量的$L_2$范数平方，定义为： \n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198049016180.jpg)\n其中， $\\hat{D^*}$ 代表预测得到的位移量(displacement volume)；$D^*$ 代表 ground-truth 的位移量。中间的点乘代表Hadamard product(哈达玛积，矩阵中对应元素相乘),$M$ 是一个binary mask，当$D^*$为0 时，$M=0$。( $D^*$ 为 0 的 entry 代表在时刻$t_m$，该位置没有行人)\n\n    由于输入数据非常稀疏，网络很容易陷入局部极小值，因此，文章采用了分层训练的方式。首先，训练一个随机初始化的三层卷积网络直到收敛，然后将这三层网络作为模型的$conv1$、$conv2$、$conv3$。之后加入后面的网络，联合训练。\n \n*  实验结果：\n采用MSE准侧函数来衡量预测结果。(文章在实现中，设置$X=Y=256$，而且文章没有提到对数据的预处理).\n对预测的$M^*$个时刻，计算预测出来的归一化位置和归一化的ground-truth之间的$L_2$距离。\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198064201861.jpg)\n上式中: $\\hat{l}_i^{M+m}=[x_i^{M+m}/x, y_i^{M+m}/y]$是归一化的位置。\n\n* Investigations on Behavior-CNN 对模型的一些实验解释\n    文章试验了有或者没有 Location bias map 对预测结果的影响。以及环境信息是否对网络预测结果有影响。\n    ![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198088088086.jpg)\n上图中的实例说明，论文提出的网络结构能够学习到位置信息以及环境信息。(a~c)说明了网络能够捕获不同区域不同的运动模式.\n\n    下图展示了不同的滤波器学到的内容：\n    ![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198094043936.jpg)\n    图(a)分布表示第一个卷积层的33、59号滤波器得到的feather map。图(b)所示为图(a)上的高响应点的轨迹。(红点代表当前位置),可以看到filter 33能够找出向下移动的行人，而filter 59可以识别向上移动的行人。层数越高的滤波器越能更好的识别某一类轨迹的行人。(图d、图e) \n对于高层次的卷积(conv4~conv6)，越能捕获复杂的运动模式。上图(f)表示第四层的filter 19能够捕获静止的行人。\n论文还讨论不同的网络结构对预测结果的影响。尤其是感受野(Receptive Fields，表明了考虑与当前行人多元范围内的行人，论文使用的是场景大小的10%)的大小对性能的影响（所以，本文的模型使用了pool层，通过pool层，使得在网络参数个数相同时，感受野为2倍）。\n\n* 结果展示：\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198112216643.jpg)\n上图中，绿色是ground-truth,红色是预测出来的。蓝色是走过的。\n* 应用：\n\n     1. 行人运动目的地预测。\n    ![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198287607930.jpg)\n\n     2. 作为先验知识提高Tracking算法的效果。\n\n","source":"_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks.md","raw":"---\ntitle: Pedestrian Behavior Understanding and Prediction with Deep Neural Networks\nmathjax: true\ndate: 2018-03-10 21:02:22\ntags: paper\n---\n# [Pedestrian Behavior Understanding and Prediction with Deep Neural Networks](https://link.springer.com/chapter/10.1007/978-3-319-46448-0_16)\n1. 论文发表在ECCV2016，使用深层神经网络理解并预测行人运动行为。(一作：Shuai Yi)\n\n2. 论文的核心思想：通过一种编码方案(encoding scheme)，使得行人的运动轨迹适合作为CNN的输入与输出。\n\n3. 论文的贡献：\n\n    > 1. 用深层卷积网络(deep-CNN)建模了行人长期运动行为(Long-term pedestrian behaviors)，并对得到的CNN的结构进行了分析。\n\n    > 2. 提出了一个行人运动行为的编码方案(pedestrian behavior encoding scheme), 将行人的运动路径编码为“稀疏位移量”(sparse displacement volumes)。后者可以直接作为深层网络的输入和输出。\n \n4. 论文的方法：\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197889166956.jpg)\n上图所示为总体的框架: \n\n    >系统的输入为前几帧行人运动轨迹(图a 中的带颜色曲线)；然后将它们编码 为位移量(displacement volume)，如图 b 所示；将编码之后的量输入到Behavior CNN 中，网络的输出也为displacement volume，然后通过解码操作得到未来的预测估计。\n\n* 运动轨迹编码：\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197894945788.jpg)\n如上图所示为编码过程。\n\n    > 令 $p_1,p_2,...,p_N$代表场景中的$N$个行人, $t_1,t_2,...,t_m$代表$m$ 个时刻，其中$t_m$代表当前时刻。在时刻$t_m$，行人 $p_i(i\\in[1,N])$ 的归一化空间位置记为$l_i^m=[x^m_i/X, y^m_i/Y]$。其中，$x_i^m\\in[1,X]$, $y_i^m\\in[1,Y]$ 为空间坐标，$[X,Y]$ 为输入帧的空间大小(spatial size)。所有的位置都是基于网格的。\n  \n    > 使用一个 $2M$ 维的位移向量$d_i=[l_i^M-l^1_i ,l^M_i-   l_i^2,...,l_i^M-l_i^{M-1},l_i^M-l_i^M]$ 表示之前$M$帧，行人 $p_i$ 的轨迹信息。$d_i \\in (-1,1)$.\n    \n    > 基于$d_i$构建一个三维的位移量tensor $D\\in R^{X\\times Y\\times 2M}$。对于每一个行人 $p_i$ ，若其当前所在时刻的位置$(x_i^M, y_i^M)$，则令 $D(x_i^M,y_i^M,:) = d_i$。为了区分含有静止行人和没有行人的位置(格子)，对所有含有行人的位置的 $d_i+\\textbf{1}^T$。即 $D(x_i^M, y_i^M,:)=d_i+ \\textbf{1}^T$。此时，含有人的格子，其$2M$个向量元素取值范围 $\\in(0,2)$, 静止行人为 1 ，$D$ 中的其余部分均设置为 0。\n    \n* CNN 模型\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197998463524.jpg)\n\n    >1. CNN 的输入为$D\\in R^{X\\times Y \\times 2M}$，输出为 $D^* \\in R^{X\\times Y \\times 2M^*}$.\n\n    >2. 经过前三个卷积层操作之后的输出大小为 $X\\times Y \\times 64$。(为了保证feather map 的大小不变，中间有zero pad 操作)\n\n    >3. 经过max-pool 操作之后(stride 为2)，输出大小为$X/2 \\times Y/2 \\times 64$。\n\n    >4. 设置一个可学习的位置偏差图(location bias map)，其大小为$X/2 \\times Y/2$。然后将这个位置偏差图 channel-wisely (总共有64个channel)加到 pool 之后得到的feather map 上。这样，每一个位置都有一个独立的偏差值(bias)，这个值在相同位置的所有通道上共享。 这个 location bias map 表示了环境信息。\n\n    >5. 之后是三个卷基层，再接着是一个逆卷积层，得到$D^*$.\n\n* 损失函数和训练：\n损失函数计算预测得到的位移量和实际的位移量的$L_2$范数平方，定义为： \n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198049016180.jpg)\n其中， $\\hat{D^*}$ 代表预测得到的位移量(displacement volume)；$D^*$ 代表 ground-truth 的位移量。中间的点乘代表Hadamard product(哈达玛积，矩阵中对应元素相乘),$M$ 是一个binary mask，当$D^*$为0 时，$M=0$。( $D^*$ 为 0 的 entry 代表在时刻$t_m$，该位置没有行人)\n\n    由于输入数据非常稀疏，网络很容易陷入局部极小值，因此，文章采用了分层训练的方式。首先，训练一个随机初始化的三层卷积网络直到收敛，然后将这三层网络作为模型的$conv1$、$conv2$、$conv3$。之后加入后面的网络，联合训练。\n \n*  实验结果：\n采用MSE准侧函数来衡量预测结果。(文章在实现中，设置$X=Y=256$，而且文章没有提到对数据的预处理).\n对预测的$M^*$个时刻，计算预测出来的归一化位置和归一化的ground-truth之间的$L_2$距离。\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198064201861.jpg)\n上式中: $\\hat{l}_i^{M+m}=[x_i^{M+m}/x, y_i^{M+m}/y]$是归一化的位置。\n\n* Investigations on Behavior-CNN 对模型的一些实验解释\n    文章试验了有或者没有 Location bias map 对预测结果的影响。以及环境信息是否对网络预测结果有影响。\n    ![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198088088086.jpg)\n上图中的实例说明，论文提出的网络结构能够学习到位置信息以及环境信息。(a~c)说明了网络能够捕获不同区域不同的运动模式.\n\n    下图展示了不同的滤波器学到的内容：\n    ![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198094043936.jpg)\n    图(a)分布表示第一个卷积层的33、59号滤波器得到的feather map。图(b)所示为图(a)上的高响应点的轨迹。(红点代表当前位置),可以看到filter 33能够找出向下移动的行人，而filter 59可以识别向上移动的行人。层数越高的滤波器越能更好的识别某一类轨迹的行人。(图d、图e) \n对于高层次的卷积(conv4~conv6)，越能捕获复杂的运动模式。上图(f)表示第四层的filter 19能够捕获静止的行人。\n论文还讨论不同的网络结构对预测结果的影响。尤其是感受野(Receptive Fields，表明了考虑与当前行人多元范围内的行人，论文使用的是场景大小的10%)的大小对性能的影响（所以，本文的模型使用了pool层，通过pool层，使得在网络参数个数相同时，感受野为2倍）。\n\n* 结果展示：\n![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198112216643.jpg)\n上图中，绿色是ground-truth,红色是预测出来的。蓝色是走过的。\n* 应用：\n\n     1. 行人运动目的地预测。\n    ![](Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198287607930.jpg)\n\n     2. 作为先验知识提高Tracking算法的效果。\n\n","slug":"Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks","published":1,"updated":"2018-03-10T13:32:08.864Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjf3w0pw20000vldxlsgafz32"},{"title":"Social LSTM","mathjax":true,"date":"2018-03-10T11:43:02.000Z","_content":"# [Social LSTM:Human Trajectory Prediction in Crowded Space](http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf)\n1. 论文发表在CVPR2016上(spotlight)(一作：Alexandre Alahi)。\n\n2. 论文的核心思想： \n![](/Social-LSTM/15203542098731.jpg)\n\n\n    将行人的轨迹预测问题看作是序列生成问题，对每一个行人使用一个LSTM网络。并且对于空间上临近的行人，引入了一个“Social pooling”层。通过这个\"Social pooling\"层，使得临近的行人能够共享他们的隐含状态 (这里的隐含状态指每个轨迹的LSTM中的隐含状态)。\n3. 论文的创新点：完全的 data-driven 方式，没有人工提取的特征，没有设计的吸引力和排斥力(social force model)；同时考虑了相关的多个序列；建模了不同序列之间的相关性。\n4. 论文的方法：\n    \n    问题规约： 对于时刻 $t$，$i^{th}$ 的坐标为$(x_t^i ,y_t^i )$。已经知道了行人时刻 1 到时刻 $T_{obs}$的坐标，希望知道$T_{obs+1}$ 到 $T_{pred}$ 的坐标。\n    ![](/Social-LSTM/15197217698948.jpg)\n\n如上图所示，在每一个time-step，LSTM cell 从临近的LSTM cell接受   共享的隐含状态信息(pooled hidden-state information),得到一个三维的tensor(前两维为平面坐标，第三个维度是隐含状态)。\n1.  LSTM 的隐含状态  $h$$_{i}^t$捕获在时刻 t，第 i 个行人的隐含状态(latent representation)。\n2. 通过构建隐含状态张量(\"Social\" hidden-state tensor) $H$$_{i}^t$与邻居共享行人隐含状态：\n\n> 给定隐含状态维度为$D$，邻居大小(即考虑范围)为$N_0$，对第 $i$ 个行人的轨迹构建一个 $N_0\\times N_0 \\times D$ 的tensor:\n![](/Social-LSTM/15197416749439.jpg)\n上式中：\n* $h$$_{t-1}^j$ 是第 j 个行人在时刻 t-1 对应的LSTM 隐含状态。\n* $1_{mn}[x, y]$是指示函数，检查 $(x, y)$是否在$(m, n)$表示的方格内部。\n* $\\mathscr{N}$$_i$ 是第 $i$ 个行人的邻居集合。\n如下图所示为黑色点代表的行人的Social-pooling\n![](/Social-LSTM/15197406060633.jpg)\n上图中表示在某个特定的空间距离内(即上式中表示的N<sub>0</sub>)的存在三个邻居，分别用黄色、蓝色、橘色表示。\n结合上图，可以理解方程(1)的含义： 首先，确定第 $i$ 个行人周围某个范围内的行人集合，然后以当前考虑行人为中心，确定各个邻居落在那个方格内，对于落在相同格子里的人，将其隐含状态相加。(上图中的示例 $N_0$等于$2$，而实验中实际$N_0$取的是32),得到tensor.\n\n3.将张量$H_t^i$映射(embed)到向量 $a_t^i$, 将坐标映射到$e_t^i$ 。然后将这些向量连接起来，作为LSTM的输入。(这里需要注意的是在构建tensort时，当前考虑的行人的隐含状态是不包括在内的，此部分状态单独作为一部分输入到LSTM中，如图一所示)\n![](/Social-LSTM/15197428867541.jpg)\n上式中$\\phi$为ReLU映射函数，$W_e$ 和 $W_a$ 为映射权重。\n\n位置估计：\n使用 $t$ 时刻的隐含状态预测 $t+1$ 时刻位置坐标的分布($\\hat{x}$, $\\hat{y}$)$_{t+1}^i$。论文中使用的概率模型为二维高斯分布, ($\\hat{x}$, $\\hat{y}$)$_{t}^i$ ~ $\\mathscr{N}(\\mu^i_t,\\sigma^i_t,\\rho^i_t)$ 。其参数如下所示：\n\n> 均值为 $\\mu^i_{t+1}=(\\mu_x,\\mu_y)^i_{t+1}$\n\n> 标准差为 $\\sigma^i_{t+1}=(\\sigma_x,\\sigma_y)^i_{t+1}$\n\n> 相关系数为 $\\rho^i_{t+1}$\n\n这些参数通过一个$5 \\times D$的权重矩阵$W_p$线性得到： \n    <center>$$[\\mu_t^i,\\sigma^i_t,\\rho^i_t] = W_p \\times h_i^{t-1} $$ </center>\nLSTM 的参数通过最小化负数对数似然损失学得($L^i$表示第 $i$ 个人的轨迹):\n![](/Social-LSTM/15197479345650.jpg)\n（这里左边括号中应该还包括$W_a$, 原文中应该是漏掉了）\n通过对训练集中所有的轨迹最小化这个损失学得参数。(论文中关于实现细节讲的很少，但是这篇文章公开了code.)\n\n实验部分：选用的两个数据集分别是[ETH](http://www.vision.ee.ethz.ch/en/datasets/) 和 [UCY](https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data), 使用三个不同的度量准则：\n\n> 1. 平均偏移错误(Average displacement error)。所有预测的坐标和真实的坐标之间的MSE。\n\n> 2. 最终偏移错误(Final displacement error)。模型预测的终点坐标和真实的终点坐标之间的差距。\n\n> 3. 非线性区域平均偏移错误(Average non-linear displacement error)。由于大部分错误都发生在转弯时，因此，文章专门衡量了在这些区域的错误率。\n\n> 实验结果如下图所示(均为错误率)(本文的对比实验也比较充分)：\n\n![](/Social-LSTM/15197514964871.jpg)\n\n\n\n\n    \n\n \n\n","source":"_posts/Social-LSTM.md","raw":"---\ntitle: Social LSTM\nmathjax: true\ndate: 2018-03-10 19:43:02\ntags: paper\n---\n# [Social LSTM:Human Trajectory Prediction in Crowded Space](http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf)\n1. 论文发表在CVPR2016上(spotlight)(一作：Alexandre Alahi)。\n\n2. 论文的核心思想： \n![](/Social-LSTM/15203542098731.jpg)\n\n\n    将行人的轨迹预测问题看作是序列生成问题，对每一个行人使用一个LSTM网络。并且对于空间上临近的行人，引入了一个“Social pooling”层。通过这个\"Social pooling\"层，使得临近的行人能够共享他们的隐含状态 (这里的隐含状态指每个轨迹的LSTM中的隐含状态)。\n3. 论文的创新点：完全的 data-driven 方式，没有人工提取的特征，没有设计的吸引力和排斥力(social force model)；同时考虑了相关的多个序列；建模了不同序列之间的相关性。\n4. 论文的方法：\n    \n    问题规约： 对于时刻 $t$，$i^{th}$ 的坐标为$(x_t^i ,y_t^i )$。已经知道了行人时刻 1 到时刻 $T_{obs}$的坐标，希望知道$T_{obs+1}$ 到 $T_{pred}$ 的坐标。\n    ![](/Social-LSTM/15197217698948.jpg)\n\n如上图所示，在每一个time-step，LSTM cell 从临近的LSTM cell接受   共享的隐含状态信息(pooled hidden-state information),得到一个三维的tensor(前两维为平面坐标，第三个维度是隐含状态)。\n1.  LSTM 的隐含状态  $h$$_{i}^t$捕获在时刻 t，第 i 个行人的隐含状态(latent representation)。\n2. 通过构建隐含状态张量(\"Social\" hidden-state tensor) $H$$_{i}^t$与邻居共享行人隐含状态：\n\n> 给定隐含状态维度为$D$，邻居大小(即考虑范围)为$N_0$，对第 $i$ 个行人的轨迹构建一个 $N_0\\times N_0 \\times D$ 的tensor:\n![](/Social-LSTM/15197416749439.jpg)\n上式中：\n* $h$$_{t-1}^j$ 是第 j 个行人在时刻 t-1 对应的LSTM 隐含状态。\n* $1_{mn}[x, y]$是指示函数，检查 $(x, y)$是否在$(m, n)$表示的方格内部。\n* $\\mathscr{N}$$_i$ 是第 $i$ 个行人的邻居集合。\n如下图所示为黑色点代表的行人的Social-pooling\n![](/Social-LSTM/15197406060633.jpg)\n上图中表示在某个特定的空间距离内(即上式中表示的N<sub>0</sub>)的存在三个邻居，分别用黄色、蓝色、橘色表示。\n结合上图，可以理解方程(1)的含义： 首先，确定第 $i$ 个行人周围某个范围内的行人集合，然后以当前考虑行人为中心，确定各个邻居落在那个方格内，对于落在相同格子里的人，将其隐含状态相加。(上图中的示例 $N_0$等于$2$，而实验中实际$N_0$取的是32),得到tensor.\n\n3.将张量$H_t^i$映射(embed)到向量 $a_t^i$, 将坐标映射到$e_t^i$ 。然后将这些向量连接起来，作为LSTM的输入。(这里需要注意的是在构建tensort时，当前考虑的行人的隐含状态是不包括在内的，此部分状态单独作为一部分输入到LSTM中，如图一所示)\n![](/Social-LSTM/15197428867541.jpg)\n上式中$\\phi$为ReLU映射函数，$W_e$ 和 $W_a$ 为映射权重。\n\n位置估计：\n使用 $t$ 时刻的隐含状态预测 $t+1$ 时刻位置坐标的分布($\\hat{x}$, $\\hat{y}$)$_{t+1}^i$。论文中使用的概率模型为二维高斯分布, ($\\hat{x}$, $\\hat{y}$)$_{t}^i$ ~ $\\mathscr{N}(\\mu^i_t,\\sigma^i_t,\\rho^i_t)$ 。其参数如下所示：\n\n> 均值为 $\\mu^i_{t+1}=(\\mu_x,\\mu_y)^i_{t+1}$\n\n> 标准差为 $\\sigma^i_{t+1}=(\\sigma_x,\\sigma_y)^i_{t+1}$\n\n> 相关系数为 $\\rho^i_{t+1}$\n\n这些参数通过一个$5 \\times D$的权重矩阵$W_p$线性得到： \n    <center>$$[\\mu_t^i,\\sigma^i_t,\\rho^i_t] = W_p \\times h_i^{t-1} $$ </center>\nLSTM 的参数通过最小化负数对数似然损失学得($L^i$表示第 $i$ 个人的轨迹):\n![](/Social-LSTM/15197479345650.jpg)\n（这里左边括号中应该还包括$W_a$, 原文中应该是漏掉了）\n通过对训练集中所有的轨迹最小化这个损失学得参数。(论文中关于实现细节讲的很少，但是这篇文章公开了code.)\n\n实验部分：选用的两个数据集分别是[ETH](http://www.vision.ee.ethz.ch/en/datasets/) 和 [UCY](https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data), 使用三个不同的度量准则：\n\n> 1. 平均偏移错误(Average displacement error)。所有预测的坐标和真实的坐标之间的MSE。\n\n> 2. 最终偏移错误(Final displacement error)。模型预测的终点坐标和真实的终点坐标之间的差距。\n\n> 3. 非线性区域平均偏移错误(Average non-linear displacement error)。由于大部分错误都发生在转弯时，因此，文章专门衡量了在这些区域的错误率。\n\n> 实验结果如下图所示(均为错误率)(本文的对比实验也比较充分)：\n\n![](/Social-LSTM/15197514964871.jpg)\n\n\n\n\n    \n\n \n\n","slug":"Social-LSTM","published":1,"updated":"2018-03-10T14:02:25.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjf3w0pw50001vldx3jl9rlmp","content":"<h1 id=\"social-lstmhuman-trajectory-prediction-in-crowded-space\"><a href=\"http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf\" target=\"_blank\" rel=\"noopener\">Social LSTM:Human Trajectory Prediction in Crowded Space</a></h1>\n<ol style=\"list-style-type: decimal\">\n<li><p>论文发表在CVPR2016上(spotlight)(一作：Alexandre Alahi)。</p></li>\n<li><p>论文的核心思想： <img src=\"/2018/03/10/Social-LSTM/15203542098731.jpg\"></p>\n将行人的轨迹预测问题看作是序列生成问题，对每一个行人使用一个LSTM网络。并且对于空间上临近的行人，引入了一个“Social pooling”层。通过这个&quot;Social pooling&quot;层，使得临近的行人能够共享他们的隐含状态 (这里的隐含状态指每个轨迹的LSTM中的隐含状态)。</li>\n<li>论文的创新点：完全的 data-driven 方式，没有人工提取的特征，没有设计的吸引力和排斥力(social force model)；同时考虑了相关的多个序列；建模了不同序列之间的相关性。</li>\n<li><p>论文的方法：</p>\n<p>问题规约： 对于时刻 <span class=\"math inline\">\\(t\\)</span>，<span class=\"math inline\">\\(i^{th}\\)</span> 的坐标为<span class=\"math inline\">\\((x_t^i ,y_t^i )\\)</span>。已经知道了行人时刻 1 到时刻 <span class=\"math inline\">\\(T_{obs}\\)</span>的坐标，希望知道<span class=\"math inline\">\\(T_{obs+1}\\)</span> 到 <span class=\"math inline\">\\(T_{pred}\\)</span> 的坐标。 <img src=\"/2018/03/10/Social-LSTM/15197217698948.jpg\"></p></li>\n</ol>\n<p>如上图所示，在每一个time-step，LSTM cell 从临近的LSTM cell接受 共享的隐含状态信息(pooled hidden-state information),得到一个三维的tensor(前两维为平面坐标，第三个维度是隐含状态)。 1. LSTM 的隐含状态 <span class=\"math inline\">\\(h\\)</span><span class=\"math inline\">\\(_{i}^t\\)</span>捕获在时刻 t，第 i 个行人的隐含状态(latent representation)。 2. 通过构建隐含状态张量(&quot;Social&quot; hidden-state tensor) <span class=\"math inline\">\\(H\\)</span><span class=\"math inline\">\\(_{i}^t\\)</span>与邻居共享行人隐含状态：</p>\n<blockquote>\n<p>给定隐含状态维度为<span class=\"math inline\">\\(D\\)</span>，邻居大小(即考虑范围)为<span class=\"math inline\">\\(N_0\\)</span>，对第 <span class=\"math inline\">\\(i\\)</span> 个行人的轨迹构建一个 <span class=\"math inline\">\\(N_0\\times N_0 \\times D\\)</span> 的tensor: <img src=\"/2018/03/10/Social-LSTM/15197416749439.jpg\"> 上式中： * <span class=\"math inline\">\\(h\\)</span><span class=\"math inline\">\\(_{t-1}^j\\)</span> 是第 j 个行人在时刻 t-1 对应的LSTM 隐含状态。 * <span class=\"math inline\">\\(1_{mn}[x, y]\\)</span>是指示函数，检查 <span class=\"math inline\">\\((x, y)\\)</span>是否在<span class=\"math inline\">\\((m, n)\\)</span>表示的方格内部。 * <span class=\"math inline\">\\(\\mathscr{N}\\)</span><span class=\"math inline\">\\(_i\\)</span> 是第 <span class=\"math inline\">\\(i\\)</span> 个行人的邻居集合。 如下图所示为黑色点代表的行人的Social-pooling <img src=\"/2018/03/10/Social-LSTM/15197406060633.jpg\"> 上图中表示在某个特定的空间距离内(即上式中表示的N<sub>0</sub>)的存在三个邻居，分别用黄色、蓝色、橘色表示。 结合上图，可以理解方程(1)的含义： 首先，确定第 <span class=\"math inline\">\\(i\\)</span> 个行人周围某个范围内的行人集合，然后以当前考虑行人为中心，确定各个邻居落在那个方格内，对于落在相同格子里的人，将其隐含状态相加。(上图中的示例 <span class=\"math inline\">\\(N_0\\)</span>等于<span class=\"math inline\">\\(2\\)</span>，而实验中实际<span class=\"math inline\">\\(N_0\\)</span>取的是32),得到tensor.</p>\n</blockquote>\n<p>3.将张量<span class=\"math inline\">\\(H_t^i\\)</span>映射(embed)到向量 <span class=\"math inline\">\\(a_t^i\\)</span>, 将坐标映射到<span class=\"math inline\">\\(e_t^i\\)</span> 。然后将这些向量连接起来，作为LSTM的输入。(这里需要注意的是在构建tensort时，当前考虑的行人的隐含状态是不包括在内的，此部分状态单独作为一部分输入到LSTM中，如图一所示) <img src=\"/2018/03/10/Social-LSTM/15197428867541.jpg\"> 上式中<span class=\"math inline\">\\(\\phi\\)</span>为ReLU映射函数，<span class=\"math inline\">\\(W_e\\)</span> 和 <span class=\"math inline\">\\(W_a\\)</span> 为映射权重。</p>\n<p>位置估计： 使用 <span class=\"math inline\">\\(t\\)</span> 时刻的隐含状态预测 <span class=\"math inline\">\\(t+1\\)</span> 时刻位置坐标的分布(<span class=\"math inline\">\\(\\hat{x}\\)</span>, <span class=\"math inline\">\\(\\hat{y}\\)</span>)<span class=\"math inline\">\\(_{t+1}^i\\)</span>。论文中使用的概率模型为二维高斯分布, (<span class=\"math inline\">\\(\\hat{x}\\)</span>, <span class=\"math inline\">\\(\\hat{y}\\)</span>)<span class=\"math inline\">\\(_{t}^i\\)</span> ~ <span class=\"math inline\">\\(\\mathscr{N}(\\mu^i_t,\\sigma^i_t,\\rho^i_t)\\)</span> 。其参数如下所示：</p>\n<blockquote>\n<p>均值为 <span class=\"math inline\">\\(\\mu^i_{t+1}=(\\mu_x,\\mu_y)^i_{t+1}\\)</span></p>\n</blockquote>\n<blockquote>\n<p>标准差为 <span class=\"math inline\">\\(\\sigma^i_{t+1}=(\\sigma_x,\\sigma_y)^i_{t+1}\\)</span></p>\n</blockquote>\n<blockquote>\n<p>相关系数为 <span class=\"math inline\">\\(\\rho^i_{t+1}\\)</span></p>\n</blockquote>\n这些参数通过一个<span class=\"math inline\">\\(5 \\times D\\)</span>的权重矩阵<span class=\"math inline\">\\(W_p\\)</span>线性得到：\n<center>\n<span class=\"math display\">\\[[\\mu_t^i,\\sigma^i_t,\\rho^i_t] = W_p \\times h_i^{t-1} \\]</span>\n</center>\n<p>LSTM 的参数通过最小化负数对数似然损失学得(<span class=\"math inline\">\\(L^i\\)</span>表示第 <span class=\"math inline\">\\(i\\)</span> 个人的轨迹): <img src=\"/2018/03/10/Social-LSTM/15197479345650.jpg\"> （这里左边括号中应该还包括<span class=\"math inline\">\\(W_a\\)</span>, 原文中应该是漏掉了） 通过对训练集中所有的轨迹最小化这个损失学得参数。(论文中关于实现细节讲的很少，但是这篇文章公开了code.)</p>\n<p>实验部分：选用的两个数据集分别是<a href=\"http://www.vision.ee.ethz.ch/en/datasets/\" target=\"_blank\" rel=\"noopener\">ETH</a> 和 <a href=\"https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data\" target=\"_blank\" rel=\"noopener\">UCY</a>, 使用三个不同的度量准则：</p>\n<blockquote>\n<ol style=\"list-style-type: decimal\">\n<li>平均偏移错误(Average displacement error)。所有预测的坐标和真实的坐标之间的MSE。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"2\" style=\"list-style-type: decimal\">\n<li>最终偏移错误(Final displacement error)。模型预测的终点坐标和真实的终点坐标之间的差距。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"3\" style=\"list-style-type: decimal\">\n<li>非线性区域平均偏移错误(Average non-linear displacement error)。由于大部分错误都发生在转弯时，因此，文章专门衡量了在这些区域的错误率。</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>实验结果如下图所示(均为错误率)(本文的对比实验也比较充分)：</p>\n</blockquote>\n<div class=\"figure\">\n<img src=\"/2018/03/10/Social-LSTM/15197514964871.jpg\">\n\n</div>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"social-lstmhuman-trajectory-prediction-in-crowded-space\"><a href=\"http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf\" target=\"_blank\" rel=\"noopener\">Social LSTM:Human Trajectory Prediction in Crowded Space</a></h1>\n<ol style=\"list-style-type: decimal\">\n<li><p>论文发表在CVPR2016上(spotlight)(一作：Alexandre Alahi)。</p></li>\n<li><p>论文的核心思想： <img src=\"/2018/03/10/Social-LSTM/15203542098731.jpg\"></p>\n将行人的轨迹预测问题看作是序列生成问题，对每一个行人使用一个LSTM网络。并且对于空间上临近的行人，引入了一个“Social pooling”层。通过这个&quot;Social pooling&quot;层，使得临近的行人能够共享他们的隐含状态 (这里的隐含状态指每个轨迹的LSTM中的隐含状态)。</li>\n<li>论文的创新点：完全的 data-driven 方式，没有人工提取的特征，没有设计的吸引力和排斥力(social force model)；同时考虑了相关的多个序列；建模了不同序列之间的相关性。</li>\n<li><p>论文的方法：</p>\n<p>问题规约： 对于时刻 <span class=\"math inline\">\\(t\\)</span>，<span class=\"math inline\">\\(i^{th}\\)</span> 的坐标为<span class=\"math inline\">\\((x_t^i ,y_t^i )\\)</span>。已经知道了行人时刻 1 到时刻 <span class=\"math inline\">\\(T_{obs}\\)</span>的坐标，希望知道<span class=\"math inline\">\\(T_{obs+1}\\)</span> 到 <span class=\"math inline\">\\(T_{pred}\\)</span> 的坐标。 <img src=\"/2018/03/10/Social-LSTM/15197217698948.jpg\"></p></li>\n</ol>\n<p>如上图所示，在每一个time-step，LSTM cell 从临近的LSTM cell接受 共享的隐含状态信息(pooled hidden-state information),得到一个三维的tensor(前两维为平面坐标，第三个维度是隐含状态)。 1. LSTM 的隐含状态 <span class=\"math inline\">\\(h\\)</span><span class=\"math inline\">\\(_{i}^t\\)</span>捕获在时刻 t，第 i 个行人的隐含状态(latent representation)。 2. 通过构建隐含状态张量(&quot;Social&quot; hidden-state tensor) <span class=\"math inline\">\\(H\\)</span><span class=\"math inline\">\\(_{i}^t\\)</span>与邻居共享行人隐含状态：</p>\n<blockquote>\n<p>给定隐含状态维度为<span class=\"math inline\">\\(D\\)</span>，邻居大小(即考虑范围)为<span class=\"math inline\">\\(N_0\\)</span>，对第 <span class=\"math inline\">\\(i\\)</span> 个行人的轨迹构建一个 <span class=\"math inline\">\\(N_0\\times N_0 \\times D\\)</span> 的tensor: <img src=\"/2018/03/10/Social-LSTM/15197416749439.jpg\"> 上式中： * <span class=\"math inline\">\\(h\\)</span><span class=\"math inline\">\\(_{t-1}^j\\)</span> 是第 j 个行人在时刻 t-1 对应的LSTM 隐含状态。 * <span class=\"math inline\">\\(1_{mn}[x, y]\\)</span>是指示函数，检查 <span class=\"math inline\">\\((x, y)\\)</span>是否在<span class=\"math inline\">\\((m, n)\\)</span>表示的方格内部。 * <span class=\"math inline\">\\(\\mathscr{N}\\)</span><span class=\"math inline\">\\(_i\\)</span> 是第 <span class=\"math inline\">\\(i\\)</span> 个行人的邻居集合。 如下图所示为黑色点代表的行人的Social-pooling <img src=\"/2018/03/10/Social-LSTM/15197406060633.jpg\"> 上图中表示在某个特定的空间距离内(即上式中表示的N<sub>0</sub>)的存在三个邻居，分别用黄色、蓝色、橘色表示。 结合上图，可以理解方程(1)的含义： 首先，确定第 <span class=\"math inline\">\\(i\\)</span> 个行人周围某个范围内的行人集合，然后以当前考虑行人为中心，确定各个邻居落在那个方格内，对于落在相同格子里的人，将其隐含状态相加。(上图中的示例 <span class=\"math inline\">\\(N_0\\)</span>等于<span class=\"math inline\">\\(2\\)</span>，而实验中实际<span class=\"math inline\">\\(N_0\\)</span>取的是32),得到tensor.</p>\n</blockquote>\n<p>3.将张量<span class=\"math inline\">\\(H_t^i\\)</span>映射(embed)到向量 <span class=\"math inline\">\\(a_t^i\\)</span>, 将坐标映射到<span class=\"math inline\">\\(e_t^i\\)</span> 。然后将这些向量连接起来，作为LSTM的输入。(这里需要注意的是在构建tensort时，当前考虑的行人的隐含状态是不包括在内的，此部分状态单独作为一部分输入到LSTM中，如图一所示) <img src=\"/2018/03/10/Social-LSTM/15197428867541.jpg\"> 上式中<span class=\"math inline\">\\(\\phi\\)</span>为ReLU映射函数，<span class=\"math inline\">\\(W_e\\)</span> 和 <span class=\"math inline\">\\(W_a\\)</span> 为映射权重。</p>\n<p>位置估计： 使用 <span class=\"math inline\">\\(t\\)</span> 时刻的隐含状态预测 <span class=\"math inline\">\\(t+1\\)</span> 时刻位置坐标的分布(<span class=\"math inline\">\\(\\hat{x}\\)</span>, <span class=\"math inline\">\\(\\hat{y}\\)</span>)<span class=\"math inline\">\\(_{t+1}^i\\)</span>。论文中使用的概率模型为二维高斯分布, (<span class=\"math inline\">\\(\\hat{x}\\)</span>, <span class=\"math inline\">\\(\\hat{y}\\)</span>)<span class=\"math inline\">\\(_{t}^i\\)</span> ~ <span class=\"math inline\">\\(\\mathscr{N}(\\mu^i_t,\\sigma^i_t,\\rho^i_t)\\)</span> 。其参数如下所示：</p>\n<blockquote>\n<p>均值为 <span class=\"math inline\">\\(\\mu^i_{t+1}=(\\mu_x,\\mu_y)^i_{t+1}\\)</span></p>\n</blockquote>\n<blockquote>\n<p>标准差为 <span class=\"math inline\">\\(\\sigma^i_{t+1}=(\\sigma_x,\\sigma_y)^i_{t+1}\\)</span></p>\n</blockquote>\n<blockquote>\n<p>相关系数为 <span class=\"math inline\">\\(\\rho^i_{t+1}\\)</span></p>\n</blockquote>\n这些参数通过一个<span class=\"math inline\">\\(5 \\times D\\)</span>的权重矩阵<span class=\"math inline\">\\(W_p\\)</span>线性得到：\n<center>\n<span class=\"math display\">\\[[\\mu_t^i,\\sigma^i_t,\\rho^i_t] = W_p \\times h_i^{t-1} \\]</span>\n</center>\n<p>LSTM 的参数通过最小化负数对数似然损失学得(<span class=\"math inline\">\\(L^i\\)</span>表示第 <span class=\"math inline\">\\(i\\)</span> 个人的轨迹): <img src=\"/2018/03/10/Social-LSTM/15197479345650.jpg\"> （这里左边括号中应该还包括<span class=\"math inline\">\\(W_a\\)</span>, 原文中应该是漏掉了） 通过对训练集中所有的轨迹最小化这个损失学得参数。(论文中关于实现细节讲的很少，但是这篇文章公开了code.)</p>\n<p>实验部分：选用的两个数据集分别是<a href=\"http://www.vision.ee.ethz.ch/en/datasets/\" target=\"_blank\" rel=\"noopener\">ETH</a> 和 <a href=\"https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data\" target=\"_blank\" rel=\"noopener\">UCY</a>, 使用三个不同的度量准则：</p>\n<blockquote>\n<ol style=\"list-style-type: decimal\">\n<li>平均偏移错误(Average displacement error)。所有预测的坐标和真实的坐标之间的MSE。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"2\" style=\"list-style-type: decimal\">\n<li>最终偏移错误(Final displacement error)。模型预测的终点坐标和真实的终点坐标之间的差距。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"3\" style=\"list-style-type: decimal\">\n<li>非线性区域平均偏移错误(Average non-linear displacement error)。由于大部分错误都发生在转弯时，因此，文章专门衡量了在这些区域的错误率。</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>实验结果如下图所示(均为错误率)(本文的对比实验也比较充分)：</p>\n</blockquote>\n<div class=\"figure\">\n<img src=\"/2018/03/10/Social-LSTM/15197514964871.jpg\">\n\n</div>\n"},{"title":"Understanding Pedestrian Behaviors(CVPR2015)","mathjax":true,"date":"2018-03-10T13:16:23.000Z","_content":"# [Understanding Pedestrian Behaviors from Stationary Crowd Groups](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Yi_Understanding_Pedestrian_Behaviors_2015_CVPR_paper.pdf)\n1. 论文发表在CVPR2015上(\b\b一作:Shuai Yi)。\n\n2. 论文的主要工作是：建立了一个行人运动(pedestrian behavior)的模型，其中，对静态人群是分析是关键部分。模型可以用来预测行人的轨迹；预测行人运动的终点；以及对人群中行人的运动行为分析。\n3. 论文的方法：\n\n    > 相关研究发现：静止的人群(stationary crowd groups)对行人运动的影响大于运动的人群(moving crowds)。(当运动的行人遇到运动的人群时, 行人一般只会减速，而当遇到静止的人群时，通常会改变方向)。\n    ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198897069641.jpg)\n如上图所示：场景中的静止人群既可以作为（单个人）运动的终点和起点，也可以作为障碍物或者通道。\n由于静止人群的位置分布(spatial distribution)可能随时间变化，因而引入了动态特征(dynamic variations of trafﬁc patterns)。而且当行人遇到静止人群时，即可能绕开人群，也可能穿过人群。论文建模了这些情况。\n\n* Pedestrian Behavior Modeling\n    \n    > 行人通常会选择最方便、最有效的路径到达目的地。文章提出了一个“通用场景能量图”(general scene energy map), 来表示场景中不同位置的travel difficulty。能量越高的地方表示行人越愿意经过这些地方，因而，在能量高的地方观测到行人的概率也越大。\n    \n* 通用能量图建模(General energy map modeling)\n\n    > 通用能量图从三个方面来建模：1. 场景布局 $f_{SL}$；2.移动的行人$f_{MP}$; 3. 静止的人群 $f_{SG}$。\n    > ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198918330798.jpg)\n其中，$\\theta_1,\\theta_2,\\theta_3,\\theta_4$不同项的权值向量。各项的表达式如下所示：\n\n    #####1.场景因素：\n    >  ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198919545235.jpg) \n    > 一般情况下，越是离障碍物近的地方，行人越不可能去。\n上式中，$SL$代表障碍物占据的位置，$d_1(x,SL)=min_{y\\in{SL}}\\parallel x-y \\parallel_2^2$ 衡量当前位置$x$与最近的障碍物距离。$x$ 越靠近障碍物，$f_{SL}$的值越小。\n\n    #####2.移动行人的影响：\n    > ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198927017157.jpg)\n行人总是与其他行人保持一定的距离，因此，行人周围地点的能量值较低。\n上式中，$MP_i$ 代表第$i$个移动的行人，$x_t^{MP_i}$是$MP_i$在当前时刻$t$的位置，$x_{t+1}^{MP_i}$是估计出来$t+1$时刻的位置。$d_2(x,MP_i)=(\\parallel x-x^{MP_i}_t\\parallel +\\parallel x-x^{MP_i}_{t+1} \\parallel )^2 - (\\parallel x_t^{MP_i} - x_{t+1}^{MP_i}\\parallel)^2$,衡量当前位置$x$到移动的行人$MP_i$的距离。(这个距离度量来自social-force model)。\n\n    #####3.静止人群的影响：\n    对于绕过人群的行人来说，静止的人群可以视为障碍物(等同于场景因素)；对于穿过人群的行人，需要根据人群的密度不同，设置不同的罚项。\n    ![](Understanding-Pedestrian-Behaviors-CVPR2015/15199214092427.jpg)\n其中，$SG_i$是第$i$个静止的人群区域。$d_3(x,SG_i)=min_{y\\in{SG_i}}\\parallel x-y \\parallel^2_2$衡量$x$到$SG_i$区域的距离。$d_4(SG_i)$衡量人群 $SG_i$ 的稀疏性($d_4$计算人群中行人之间的平均距离)。当$x\\in SG_i$时，$d_3(x,SG_i)=0$.\n\n* 个性化能量图(Personalized energy map modeling)\n 在相同的环境下，不同的行人也可能有不同的走法，通过设置一个个性化因子$P$来建模.\n ![](Understanding-Pedestrian-Behaviors-CVPR2015/15199222658062.jpg)\n若$P$值较大，则对于这个人来说，通用能量图中的所有因子$\\theta_1,\\theta_2,\\theta_3$将同比例增大，此时，障碍物、行人、静止的人群周围的能量值都将下降，因而，此行人将走更远的路，来避免接触到这些障碍物。而当$P$值较小时，此人较少关注这些障碍物(walking aggressively and cares less about obstacles.)\n\n* 路径生成的过程\n给定起始点$x_s$和终点$s_d$，可以基于能量图M(或者个性化能量图$M_P$)，通过Fast Marching算法得到最优路径。\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201623680549.jpg)\n此时得到的$\\hat{T}$是当前能量图下最可能、最有效的路线。\n* 模型的学习\n能量图中某一点的能量表示了在这点观察到行人的概率。因此，模型的参数可以通过最大化似然函数得到。通过一个归一化项$Z(\\theta)$，得到：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201632366350.jpg)\n其中，$Z(\\theta)= \\int M(x;\\theta)dx$。\n对于给定的K个观测点$x_1,x_2,...,x_k$，观测到序列$X=\\{x_1,x_2,...,x_k\\}$ 的似然函数为：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201639578418.jpg)\n因而参数$\\theta$可以通过下式得到：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201640415371.jpg)\n通过梯度下降法迭代更新参数：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201640785872.jpg)\n* 行人的路线预测：\n定义一个over-cost值 $\\eta$ 来衡量预测结果和ground-truth的匹配程度。\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201654272269.jpg)\n上式中$C(T_O,M)$是基于当前能量图,由实际观测到的路线产生的cost。而$C(\\hat{T},M)$是基于最优路线$\\hat{T}$产生的花费。由于$C(T_O,M) > C(\\hat{T},M)$。因而$\\eta$越小，表示匹配的越好。\n结果：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201658922276.jpg)\n* 行人运动终点预测：\n给定起点$x_s$和部分轨迹$T$，可以预测终点位置：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201660409081.jpg)\n方法是：输入起点和前半部分轨迹，对于10个终点位置$S_i$，计算\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201663728707.jpg)\n其中，$\\hat{T_{0.5}}(x_d')$是$\\hat{T}(x_d')$的前半部分，而$\\hat{T}(x_d')=f_{FM}(M, x_s, x_d')$表示从$x_s$出发到$x_d'$的最优路线，$D(·,·)$表示两条轨迹之间的距离，\ntop_n 的预测结果如下：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201667242091.jpg)\n\n原文中还包括对$P$值的探讨，以及通过$P$值将人群分类(方法还是通过最小化预测轨迹和观测轨迹的差距)。这里不再介绍。\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Understanding-Pedestrian-Behaviors-CVPR2015.md","raw":"---\ntitle: Understanding Pedestrian Behaviors(CVPR2015)\nmathjax: true\ndate: 2018-03-10 21:16:23\ntags: paper\n---\n# [Understanding Pedestrian Behaviors from Stationary Crowd Groups](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Yi_Understanding_Pedestrian_Behaviors_2015_CVPR_paper.pdf)\n1. 论文发表在CVPR2015上(\b\b一作:Shuai Yi)。\n\n2. 论文的主要工作是：建立了一个行人运动(pedestrian behavior)的模型，其中，对静态人群是分析是关键部分。模型可以用来预测行人的轨迹；预测行人运动的终点；以及对人群中行人的运动行为分析。\n3. 论文的方法：\n\n    > 相关研究发现：静止的人群(stationary crowd groups)对行人运动的影响大于运动的人群(moving crowds)。(当运动的行人遇到运动的人群时, 行人一般只会减速，而当遇到静止的人群时，通常会改变方向)。\n    ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198897069641.jpg)\n如上图所示：场景中的静止人群既可以作为（单个人）运动的终点和起点，也可以作为障碍物或者通道。\n由于静止人群的位置分布(spatial distribution)可能随时间变化，因而引入了动态特征(dynamic variations of trafﬁc patterns)。而且当行人遇到静止人群时，即可能绕开人群，也可能穿过人群。论文建模了这些情况。\n\n* Pedestrian Behavior Modeling\n    \n    > 行人通常会选择最方便、最有效的路径到达目的地。文章提出了一个“通用场景能量图”(general scene energy map), 来表示场景中不同位置的travel difficulty。能量越高的地方表示行人越愿意经过这些地方，因而，在能量高的地方观测到行人的概率也越大。\n    \n* 通用能量图建模(General energy map modeling)\n\n    > 通用能量图从三个方面来建模：1. 场景布局 $f_{SL}$；2.移动的行人$f_{MP}$; 3. 静止的人群 $f_{SG}$。\n    > ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198918330798.jpg)\n其中，$\\theta_1,\\theta_2,\\theta_3,\\theta_4$不同项的权值向量。各项的表达式如下所示：\n\n    #####1.场景因素：\n    >  ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198919545235.jpg) \n    > 一般情况下，越是离障碍物近的地方，行人越不可能去。\n上式中，$SL$代表障碍物占据的位置，$d_1(x,SL)=min_{y\\in{SL}}\\parallel x-y \\parallel_2^2$ 衡量当前位置$x$与最近的障碍物距离。$x$ 越靠近障碍物，$f_{SL}$的值越小。\n\n    #####2.移动行人的影响：\n    > ![](Understanding-Pedestrian-Behaviors-CVPR2015/15198927017157.jpg)\n行人总是与其他行人保持一定的距离，因此，行人周围地点的能量值较低。\n上式中，$MP_i$ 代表第$i$个移动的行人，$x_t^{MP_i}$是$MP_i$在当前时刻$t$的位置，$x_{t+1}^{MP_i}$是估计出来$t+1$时刻的位置。$d_2(x,MP_i)=(\\parallel x-x^{MP_i}_t\\parallel +\\parallel x-x^{MP_i}_{t+1} \\parallel )^2 - (\\parallel x_t^{MP_i} - x_{t+1}^{MP_i}\\parallel)^2$,衡量当前位置$x$到移动的行人$MP_i$的距离。(这个距离度量来自social-force model)。\n\n    #####3.静止人群的影响：\n    对于绕过人群的行人来说，静止的人群可以视为障碍物(等同于场景因素)；对于穿过人群的行人，需要根据人群的密度不同，设置不同的罚项。\n    ![](Understanding-Pedestrian-Behaviors-CVPR2015/15199214092427.jpg)\n其中，$SG_i$是第$i$个静止的人群区域。$d_3(x,SG_i)=min_{y\\in{SG_i}}\\parallel x-y \\parallel^2_2$衡量$x$到$SG_i$区域的距离。$d_4(SG_i)$衡量人群 $SG_i$ 的稀疏性($d_4$计算人群中行人之间的平均距离)。当$x\\in SG_i$时，$d_3(x,SG_i)=0$.\n\n* 个性化能量图(Personalized energy map modeling)\n 在相同的环境下，不同的行人也可能有不同的走法，通过设置一个个性化因子$P$来建模.\n ![](Understanding-Pedestrian-Behaviors-CVPR2015/15199222658062.jpg)\n若$P$值较大，则对于这个人来说，通用能量图中的所有因子$\\theta_1,\\theta_2,\\theta_3$将同比例增大，此时，障碍物、行人、静止的人群周围的能量值都将下降，因而，此行人将走更远的路，来避免接触到这些障碍物。而当$P$值较小时，此人较少关注这些障碍物(walking aggressively and cares less about obstacles.)\n\n* 路径生成的过程\n给定起始点$x_s$和终点$s_d$，可以基于能量图M(或者个性化能量图$M_P$)，通过Fast Marching算法得到最优路径。\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201623680549.jpg)\n此时得到的$\\hat{T}$是当前能量图下最可能、最有效的路线。\n* 模型的学习\n能量图中某一点的能量表示了在这点观察到行人的概率。因此，模型的参数可以通过最大化似然函数得到。通过一个归一化项$Z(\\theta)$，得到：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201632366350.jpg)\n其中，$Z(\\theta)= \\int M(x;\\theta)dx$。\n对于给定的K个观测点$x_1,x_2,...,x_k$，观测到序列$X=\\{x_1,x_2,...,x_k\\}$ 的似然函数为：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201639578418.jpg)\n因而参数$\\theta$可以通过下式得到：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201640415371.jpg)\n通过梯度下降法迭代更新参数：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201640785872.jpg)\n* 行人的路线预测：\n定义一个over-cost值 $\\eta$ 来衡量预测结果和ground-truth的匹配程度。\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201654272269.jpg)\n上式中$C(T_O,M)$是基于当前能量图,由实际观测到的路线产生的cost。而$C(\\hat{T},M)$是基于最优路线$\\hat{T}$产生的花费。由于$C(T_O,M) > C(\\hat{T},M)$。因而$\\eta$越小，表示匹配的越好。\n结果：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201658922276.jpg)\n* 行人运动终点预测：\n给定起点$x_s$和部分轨迹$T$，可以预测终点位置：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201660409081.jpg)\n方法是：输入起点和前半部分轨迹，对于10个终点位置$S_i$，计算\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201663728707.jpg)\n其中，$\\hat{T_{0.5}}(x_d')$是$\\hat{T}(x_d')$的前半部分，而$\\hat{T}(x_d')=f_{FM}(M, x_s, x_d')$表示从$x_s$出发到$x_d'$的最优路线，$D(·,·)$表示两条轨迹之间的距离，\ntop_n 的预测结果如下：\n![](Understanding-Pedestrian-Behaviors-CVPR2015/15201667242091.jpg)\n\n原文中还包括对$P$值的探讨，以及通过$P$值将人群分类(方法还是通过最小化预测轨迹和观测轨迹的差距)。这里不再介绍。\n\n\n\n\n\n\n\n\n\n\n","slug":"Understanding-Pedestrian-Behaviors-CVPR2015","published":1,"updated":"2018-03-10T13:28:36.426Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjf3w0pw80003vldxjeq6xpuo"},{"title":"You'll never walk alone","mathjax":true,"date":"2018-03-23T11:45:00.000Z","_content":"# [You'll Never Walk Alone: Modeling Social Behavior for Multi-target Tracking](http://vision.cse.psu.edu/courses/Tracking/vlpr12/PellegriniNeverWalkAlone.pdf)\n* 论文发表在 ICCV2009上，引用558次。\n\n* 文章提出了一个LTA(Linear Trajectory Avoidance)模型，来建模短期行人的行为。(The model is designed for walking people with short-term prediction in mind).\n\n##Modeling Social Behavior:\n\n1. 使用$s_i = (p_i^t, v_i^t)$ 表示第$i$个行人的状态。其中，$p_i^t$表示二维平面上的位置，$v_i^t$表示第$i$个时刻的速度(矢量)。模型根据当前时刻所有对象的位置及速度预测下一个时刻每个人的速度。====\n2. 假设每个行人都知道其他行人此刻的位置及速度。并且，每个行人都预测其他行人会保持当前的运动状态。假设行人$s_i$在当前时刻的速度为$\\tilde{v}_i$，则他与行人$s_j$在时刻$t$的距离平方为$d_{ij}^2(t)$为： $$d_{ij}^2(t,\\tilde{v_i})=\\|p_i+t\\tilde{v_i}-p_j-tv_j\\|$$\n这里$d_{ij}$依赖于$\\tilde{v_i}$，表明正在使用$s_i$的视角。\n3. 定义 $k_{ij}^t = p_i^t-p_j^t$, $q_{ij}^t =\\tilde{v}_i-v_j^t$, 则可以将上式重写为：$$d_{ij}^2(t,\\tilde{v_i})=\\|k+  tq\\|^2$$\n4. 由于$s_i$对$s_j$的速度有一个估计，因而$s_i$会调整他的速度使得两人的最小距离${d_{ij}^*}^2$大于某个阈值(comfortable distance)。假设在时刻$t^*$，$s_i$和$s_j$的距离达到这个阈值:$$t^*= \\underset{t>0}{\\arg\\min}\\ d_{ij}^2(t,\\tilde{v}_i)$$\n5. 由于$d_{ij}^2(t,\\tilde{v_i})$是$t$的二次函数，将其对$t$求偏导,并令其等于$0$得到：$$\\frac{\\partial{d_{ij}^2(t,\\tilde{v}_i)}}{\\partial{t}}=2(k+ {tq})q^{\\top}=0 \\rightarrow t^*=-\\frac{k \\cdot {q}}{\\|q\\|^2}$$\n6. 将$t^*$带入到第三步中，即可得到最近距离: $${d_{ij}^*}^2(\\tilde{v_i})=\\|k-\\frac{k\\cdot{q}}{\\|q\\|^2}q^{\\top}\\|^2$$\n\n    (${d_{ij}^*}^2(\\tilde{v_i})$ is the expected minimum distance between pedestrian $i$ and $j$ under a constant-velocity assumption)，由于是以$s_i$的视角来看待这个问题，因而假定$s_j$的各个属性是已知的。${d_{ij}^*}^2(\\tilde{v_i})$是一个关于$s_i$当前速度$\\tilde{v_i}$的函数。\n7. 为了确保$s_i$和$s_j$能够避开，可以给上式指定一个值。但是当存在多个行人时，这种做法不太方便。因而为了建模多个行人之间的互相作用，可以使用能量函数$E_{ij}$来表示行人$s_i$和$s_j$之间的相互作用。这个能量函数是${d_{ij}^*}^2$的函数：$$E_{ij}(\\tilde{v}_i)=e^{-\\frac{{d_{ij}^*}^2(\\tilde{v_i})}{2\\sigma_d^2}}$$\n上式中$\\sigma_d$控制到要避免的对象的距离(是一个要从数据中学习的参数)。当两个对象将要相撞时，$E_{ij}$变大;当${d_{ij}^*}^2$越来越大是，$E_{ij}$很小。\n8. 通过上式，多个行人对$s_i$的影响可以建模为加权和。对每个对象$s_r(r\\neq i)$,基于他当前的位置和与$s_i$的夹角$\\phi$，分配一个权重$w_r(i)$: $$w_r(i)=w_r^d(i)w_r^\\phi(i)$$ $$w_r^d(i)=e^{-\\frac{\\|k_{ir}\\|^2}{2\\sigma_w ^2}}$$ $$w_r^{\\phi}=((1+cos(\\phi))/2)^{\\beta}$$ 其中，$\\sigma_w$定义其他对象的影响半径。$\\beta$ 控制视野权重项的峰值。\n9. 对于$s_i$来说，其所有的相互作用能量(overall interaction energy)表示为: $$I_i(\\tilde{v_i})=\\sum_{r\\neq i} {w_r(i)E_{ir}(\\tilde{v_i})}$$\n10. 以上只讨论了行人之间的相互影响，没有考虑环境信息。因此，假设每一个行人都有一个目的地 $z_i$, 并且行人倾向于保持一个速度$u_i$(desired speed)。这两个因素可以被表示为两个energy potentials.$$S_i(\\tilde{v_i})=(u_i-\\|\\tilde{v_i}\\|)^2$$ $$D_i(\\tilde{v_i})=-\\frac{(z_i-p_i)\\cdot\\tilde{v_i}}{\\|z_i-p_i\\|\\cdot \\|\\tilde{v_i}\\|}$$\n11. $s_i$所有的能量(energy)可以被表示为: $$E_i(\\tilde{v_i})=I_i(\\tilde{v_i})+\\lambda_1S_i(\\tilde{v_i})+\\lambda_2D_i(\\tilde{v_i})$$\n$\\lambda_1$和$\\lambda_2$为权重。要解决的问题就是求出当$E_i(\\tilde{v_i})$取最小值时，对应的速度$\\tilde{v_i}$.(求解方法是梯度下降法)\n![](15214261731616.jpg)\n上图为能量的示意图。黑色虚线代表当前的方向，$c_1$和$c_2$代表下一时刻可能的位置。品红色点代表当前的速度(矢量),白色点表示能量最少的速度(矢量)。\n12. 下图示意的是当$s_1$遇到两个行人$s_2$和$s_3$，其能量图。中间的图纵轴$d_{23}$表示两个行人$s_2$和$s_3$之间的距离。从图的下方可以看到，当$S_2$和$s_3$相聚较远的时候，中间的能量较低，表明$s_1$可以从两人中间走过。在图片上方，$s_1$只能绕开$s_2$和$s_3$。\n![](15214282365054.jpg)\n13. 通过使能量函数最小化，可以计算下一个期望速度$\\tilde{v_i^*}$。由于惯性的作用，对象必须经过一个转换过程。对象下一时刻的位置为: $$p_i^{t_N}=p_i+(\\alpha_N v_i+(1-\\alpha_N)\\tilde{v_i^*})t_N$$ \nN为预测的间隔。$\\alpha$是一个混合系数。\n14. 对于静止的障碍。将它们视为速度为0，并且以此刻障碍上距离行人最近的点作为障碍的位置。\n15. 模型的应用：给定$t$时刻的环境,通过最小化11式的能量函数，依次推测每    个行人$t+1$时刻的最优速度。然后将结果带入到13式中。一旦所有人的速度都已经确定后，同时更新。\n\n## Training\n模型总共包含了6个参数需要学习，${\\sigma}_d$定义了comfortable distance; ${\\sigma}_w$定义了radius of interest; 另外还有参数$\\beta$，权重$\\lambda_1$、$\\lambda_2$.系数$\\alpha$。在每一轮迭代中，每个对象依次模拟，同时保持其他对象ground-truth. 得出的参数表明，大约1m 是comfortable 的分界，6m 会影响行人的运动。\n\n##Prediction\n在每一个步中，使用预测地点和ground truth 之间的平均欧拉距离作为衡量标准。作为对比的模型是：匀速模型LIN；社会力模型；完整的LTA模型；以及在训练过程中，不使用行人之间相互作用，得到的一个模型DEST(仅包含目的地项的模型)\n![](15214628479966.jpg)\n上图中，比SF和DEST 好6%,比LIN好24%.为了更清楚的比较模型之间的差异，可以观察错误的分布。为此，作如下定义: 若预测出来的位置和ground-truth 之间的距离小于一个阈值，就认为正确。左图横轴表示阈值。可以看到，在阈值为1m的时候，预测的准确率接近75%。右图是预测的示意图。\n\n\n","source":"_posts/You-ll-never-walk-alone.md","raw":"---\ntitle: You'll never walk alone\nmathjax: true\ndate: 2018-03-23 19:45:00\ntags: paper\n---\n# [You'll Never Walk Alone: Modeling Social Behavior for Multi-target Tracking](http://vision.cse.psu.edu/courses/Tracking/vlpr12/PellegriniNeverWalkAlone.pdf)\n* 论文发表在 ICCV2009上，引用558次。\n\n* 文章提出了一个LTA(Linear Trajectory Avoidance)模型，来建模短期行人的行为。(The model is designed for walking people with short-term prediction in mind).\n\n##Modeling Social Behavior:\n\n1. 使用$s_i = (p_i^t, v_i^t)$ 表示第$i$个行人的状态。其中，$p_i^t$表示二维平面上的位置，$v_i^t$表示第$i$个时刻的速度(矢量)。模型根据当前时刻所有对象的位置及速度预测下一个时刻每个人的速度。====\n2. 假设每个行人都知道其他行人此刻的位置及速度。并且，每个行人都预测其他行人会保持当前的运动状态。假设行人$s_i$在当前时刻的速度为$\\tilde{v}_i$，则他与行人$s_j$在时刻$t$的距离平方为$d_{ij}^2(t)$为： $$d_{ij}^2(t,\\tilde{v_i})=\\|p_i+t\\tilde{v_i}-p_j-tv_j\\|$$\n这里$d_{ij}$依赖于$\\tilde{v_i}$，表明正在使用$s_i$的视角。\n3. 定义 $k_{ij}^t = p_i^t-p_j^t$, $q_{ij}^t =\\tilde{v}_i-v_j^t$, 则可以将上式重写为：$$d_{ij}^2(t,\\tilde{v_i})=\\|k+  tq\\|^2$$\n4. 由于$s_i$对$s_j$的速度有一个估计，因而$s_i$会调整他的速度使得两人的最小距离${d_{ij}^*}^2$大于某个阈值(comfortable distance)。假设在时刻$t^*$，$s_i$和$s_j$的距离达到这个阈值:$$t^*= \\underset{t>0}{\\arg\\min}\\ d_{ij}^2(t,\\tilde{v}_i)$$\n5. 由于$d_{ij}^2(t,\\tilde{v_i})$是$t$的二次函数，将其对$t$求偏导,并令其等于$0$得到：$$\\frac{\\partial{d_{ij}^2(t,\\tilde{v}_i)}}{\\partial{t}}=2(k+ {tq})q^{\\top}=0 \\rightarrow t^*=-\\frac{k \\cdot {q}}{\\|q\\|^2}$$\n6. 将$t^*$带入到第三步中，即可得到最近距离: $${d_{ij}^*}^2(\\tilde{v_i})=\\|k-\\frac{k\\cdot{q}}{\\|q\\|^2}q^{\\top}\\|^2$$\n\n    (${d_{ij}^*}^2(\\tilde{v_i})$ is the expected minimum distance between pedestrian $i$ and $j$ under a constant-velocity assumption)，由于是以$s_i$的视角来看待这个问题，因而假定$s_j$的各个属性是已知的。${d_{ij}^*}^2(\\tilde{v_i})$是一个关于$s_i$当前速度$\\tilde{v_i}$的函数。\n7. 为了确保$s_i$和$s_j$能够避开，可以给上式指定一个值。但是当存在多个行人时，这种做法不太方便。因而为了建模多个行人之间的互相作用，可以使用能量函数$E_{ij}$来表示行人$s_i$和$s_j$之间的相互作用。这个能量函数是${d_{ij}^*}^2$的函数：$$E_{ij}(\\tilde{v}_i)=e^{-\\frac{{d_{ij}^*}^2(\\tilde{v_i})}{2\\sigma_d^2}}$$\n上式中$\\sigma_d$控制到要避免的对象的距离(是一个要从数据中学习的参数)。当两个对象将要相撞时，$E_{ij}$变大;当${d_{ij}^*}^2$越来越大是，$E_{ij}$很小。\n8. 通过上式，多个行人对$s_i$的影响可以建模为加权和。对每个对象$s_r(r\\neq i)$,基于他当前的位置和与$s_i$的夹角$\\phi$，分配一个权重$w_r(i)$: $$w_r(i)=w_r^d(i)w_r^\\phi(i)$$ $$w_r^d(i)=e^{-\\frac{\\|k_{ir}\\|^2}{2\\sigma_w ^2}}$$ $$w_r^{\\phi}=((1+cos(\\phi))/2)^{\\beta}$$ 其中，$\\sigma_w$定义其他对象的影响半径。$\\beta$ 控制视野权重项的峰值。\n9. 对于$s_i$来说，其所有的相互作用能量(overall interaction energy)表示为: $$I_i(\\tilde{v_i})=\\sum_{r\\neq i} {w_r(i)E_{ir}(\\tilde{v_i})}$$\n10. 以上只讨论了行人之间的相互影响，没有考虑环境信息。因此，假设每一个行人都有一个目的地 $z_i$, 并且行人倾向于保持一个速度$u_i$(desired speed)。这两个因素可以被表示为两个energy potentials.$$S_i(\\tilde{v_i})=(u_i-\\|\\tilde{v_i}\\|)^2$$ $$D_i(\\tilde{v_i})=-\\frac{(z_i-p_i)\\cdot\\tilde{v_i}}{\\|z_i-p_i\\|\\cdot \\|\\tilde{v_i}\\|}$$\n11. $s_i$所有的能量(energy)可以被表示为: $$E_i(\\tilde{v_i})=I_i(\\tilde{v_i})+\\lambda_1S_i(\\tilde{v_i})+\\lambda_2D_i(\\tilde{v_i})$$\n$\\lambda_1$和$\\lambda_2$为权重。要解决的问题就是求出当$E_i(\\tilde{v_i})$取最小值时，对应的速度$\\tilde{v_i}$.(求解方法是梯度下降法)\n![](15214261731616.jpg)\n上图为能量的示意图。黑色虚线代表当前的方向，$c_1$和$c_2$代表下一时刻可能的位置。品红色点代表当前的速度(矢量),白色点表示能量最少的速度(矢量)。\n12. 下图示意的是当$s_1$遇到两个行人$s_2$和$s_3$，其能量图。中间的图纵轴$d_{23}$表示两个行人$s_2$和$s_3$之间的距离。从图的下方可以看到，当$S_2$和$s_3$相聚较远的时候，中间的能量较低，表明$s_1$可以从两人中间走过。在图片上方，$s_1$只能绕开$s_2$和$s_3$。\n![](15214282365054.jpg)\n13. 通过使能量函数最小化，可以计算下一个期望速度$\\tilde{v_i^*}$。由于惯性的作用，对象必须经过一个转换过程。对象下一时刻的位置为: $$p_i^{t_N}=p_i+(\\alpha_N v_i+(1-\\alpha_N)\\tilde{v_i^*})t_N$$ \nN为预测的间隔。$\\alpha$是一个混合系数。\n14. 对于静止的障碍。将它们视为速度为0，并且以此刻障碍上距离行人最近的点作为障碍的位置。\n15. 模型的应用：给定$t$时刻的环境,通过最小化11式的能量函数，依次推测每    个行人$t+1$时刻的最优速度。然后将结果带入到13式中。一旦所有人的速度都已经确定后，同时更新。\n\n## Training\n模型总共包含了6个参数需要学习，${\\sigma}_d$定义了comfortable distance; ${\\sigma}_w$定义了radius of interest; 另外还有参数$\\beta$，权重$\\lambda_1$、$\\lambda_2$.系数$\\alpha$。在每一轮迭代中，每个对象依次模拟，同时保持其他对象ground-truth. 得出的参数表明，大约1m 是comfortable 的分界，6m 会影响行人的运动。\n\n##Prediction\n在每一个步中，使用预测地点和ground truth 之间的平均欧拉距离作为衡量标准。作为对比的模型是：匀速模型LIN；社会力模型；完整的LTA模型；以及在训练过程中，不使用行人之间相互作用，得到的一个模型DEST(仅包含目的地项的模型)\n![](15214628479966.jpg)\n上图中，比SF和DEST 好6%,比LIN好24%.为了更清楚的比较模型之间的差异，可以观察错误的分布。为此，作如下定义: 若预测出来的位置和ground-truth 之间的距离小于一个阈值，就认为正确。左图横轴表示阈值。可以看到，在阈值为1m的时候，预测的准确率接近75%。右图是预测的示意图。\n\n\n","slug":"You-ll-never-walk-alone","published":1,"updated":"2018-03-23T11:50:38.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjf3w0pw90004vldxtbdkpkhs"}],"PostAsset":[{"_id":"source/_posts/Social-LSTM/15197514383605.jpg","slug":"15197514383605.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197998463524.jpg","slug":"15197998463524.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197889166956.jpg","slug":"15197889166956.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201660409081.jpg","slug":"15201660409081.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15197894945788.jpg","slug":"15197894945788.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197217698948.jpg","slug":"15197217698948.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/You-ll-never-walk-alone/15214261731616.jpg","slug":"15214261731616.jpg","post":"cjf3w0pw90004vldxtbdkpkhs","modified":0,"renderable":0},{"_id":"source/_posts/You-ll-never-walk-alone/15214282365054.jpg","slug":"15214282365054.jpg","post":"cjf3w0pw90004vldxtbdkpkhs","modified":0,"renderable":0},{"_id":"source/_posts/You-ll-never-walk-alone/15214628479966.jpg","slug":"15214628479966.jpg","post":"cjf3w0pw90004vldxtbdkpkhs","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198049016180.jpg","slug":"15198049016180.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198064201861.jpg","slug":"15198064201861.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198088088086.jpg","slug":"15198088088086.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198094043936.jpg","slug":"15198094043936.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198112216643.jpg","slug":"15198112216643.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Pedestrian-Behavior-Understanding-and-Prediction-with-Deep-Neural-Networks/15198287607930.jpg","slug":"15198287607930.jpg","post":"cjf3w0pw20000vldxlsgafz32","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197392983551.jpg","slug":"15197392983551.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197406060633.jpg","slug":"15197406060633.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197416749439.jpg","slug":"15197416749439.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197428867541.jpg","slug":"15197428867541.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197479345650.jpg","slug":"15197479345650.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197501486751.jpg","slug":"15197501486751.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197501740806.jpg","slug":"15197501740806.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197511439317.png","slug":"15197511439317.png","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197511728931.png","slug":"15197511728931.png","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15197514964871.jpg","slug":"15197514964871.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/15203542098731.jpg","slug":"15203542098731.jpg","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Social-LSTM/Snipaste_2018-02-28_00-50-47.png","slug":"Snipaste_2018-02-28_00-50-47.png","post":"cjf3w0pw50001vldx3jl9rlmp","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198897069641.jpg","slug":"15198897069641.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198918330798.jpg","slug":"15198918330798.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198919545235.jpg","slug":"15198919545235.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15198927017157.jpg","slug":"15198927017157.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15199214092427.jpg","slug":"15199214092427.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15199222476098.jpg","slug":"15199222476098.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15199222658062.jpg","slug":"15199222658062.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201623680549.jpg","slug":"15201623680549.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201632366350.jpg","slug":"15201632366350.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201639578418.jpg","slug":"15201639578418.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201640415371.jpg","slug":"15201640415371.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201640785872.jpg","slug":"15201640785872.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201654272269.jpg","slug":"15201654272269.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201658922276.jpg","slug":"15201658922276.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201663728707.jpg","slug":"15201663728707.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0},{"_id":"source/_posts/Understanding-Pedestrian-Behaviors-CVPR2015/15201667242091.jpg","slug":"15201667242091.jpg","post":"cjf3w0pw80003vldxjeq6xpuo","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"cjf3w0pw90004vldxtbdkpkhs","tag_id":"cjf3w0pw70002vldx15c4h655","_id":"cjf3w0pwb0006vldxh442c7ep"},{"post_id":"cjf3w0pw20000vldxlsgafz32","tag_id":"cjf3w0pw70002vldx15c4h655","_id":"cjf3w0pwb0007vldxso5lb938"},{"post_id":"cjf3w0pw50001vldx3jl9rlmp","tag_id":"cjf3w0pw70002vldx15c4h655","_id":"cjf3w0pwc0009vldx64v5dmdt"},{"post_id":"cjf3w0pw80003vldxjeq6xpuo","tag_id":"cjf3w0pw70002vldx15c4h655","_id":"cjf3w0pwd000avldxg6zsm8aj"}],"Tag":[{"name":"paper","_id":"cjf3w0pw70002vldx15c4h655"}]}}